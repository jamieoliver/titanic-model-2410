{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25f705fb-dd83-4242-afd5-f791fe198e1c",
   "metadata": {},
   "source": [
    "# Titanic: Survival Model\n",
    "\n",
    "Build and train a model to predict survival on the Titanic using a [cleaned and split dataset](https://huggingface.co/datasets/jamieoliver/titanic-2410), and upload the model to Hugging Face.\n",
    "\n",
    "Based on https://github.com/fastai/course22/blob/master/clean/05-linear-model-and-neural-net-from-scratch.ipynb using the dataset from https://www.kaggle.com/competitions/titanic.\n",
    "\n",
    "Plan\n",
    "- [x] Download [cleaned and split dataset](https://huggingface.co/datasets/jamieoliver/titanic-2410) from Hugging Face\n",
    "- [x] Prepare data for model\n",
    "    - [x] Load training dataset as PyTorch tensors\n",
    "    - [x] Normalise training dataset\n",
    "- [x] Train linear model\n",
    "    - [x] Set up coefficients\n",
    "    - [x] Set up gradient descent step\n",
    "    - [x] Run training loop\n",
    "- [x] Train neural network\n",
    "    - [x] Set up coefficients\n",
    "    - [x] Run training loop\n",
    "- [x] Train deep neural network\n",
    "    - [x] Set up coefficients\n",
    "    - [x] Run training loop\n",
    "- [x] Recreate as PyTorch module\n",
    "- [ ] Test model\n",
    "- [ ] Upload model to Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813bd16e-9fbd-4d9d-a400-fabc8ff7f2e6",
   "metadata": {},
   "source": [
    "##  Download Dataset from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "928b3381-2e66-47ea-b937-b848141f0cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['PassengerId', 'Survived', 'Name', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'LogFare', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Sex_female', 'Sex_male', 'Embarked_C', 'Embarked_Q', 'Embarked_S'],\n",
       "        num_rows: 712\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['PassengerId', 'Survived', 'Name', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'LogFare', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Sex_female', 'Sex_male', 'Embarked_C', 'Embarked_Q', 'Embarked_S'],\n",
       "        num_rows: 179\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['PassengerId', 'Survived', 'Name', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'LogFare', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Sex_female', 'Sex_male', 'Embarked_C', 'Embarked_Q', 'Embarked_S'],\n",
       "        num_rows: 418\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import *\n",
    "\n",
    "datasetDict = load_dataset('jamieoliver/titanic-2410', revision='1.0')\n",
    "datasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50258aa3-5330-4970-a655-2e9eacd3c502",
   "metadata": {},
   "source": [
    "## Prepare Data for Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c91b29-6e0c-4952-ab70-59eb24254d5e",
   "metadata": {},
   "source": [
    "### Load Training Dataset as PyTorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d762c5e-33d5-4f62-a8a8-5b6327c26e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "\n",
    "torch.set_default_device('cuda')\n",
    "torch.set_printoptions(linewidth=120, edgeitems=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "366f362e-4d3a-4bd6-ac99-67a9ca403d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['PassengerId', 'Survived', 'Name', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'LogFare', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Sex_female', 'Sex_male', 'Embarked_C', 'Embarked_Q', 'Embarked_S'],\n",
       "    num_rows: 712\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = datasetDict['train']\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee93576a-78cf-4ce4-840b-c967435777e7",
   "metadata": {},
   "source": [
    "The dependent variable is the variable we are predicting i.e. `survived`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca8bd00a-69b5-4923-bb8e-39612871d383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1.,\n",
       "        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
       "        1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
       "        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
       "        1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
       "        1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
       "        0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
       "        1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
       "        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
       "        1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
       "        1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
       "        1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
       "        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0.,\n",
       "        1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
       "        0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0.], device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependent_var = tensor(train_dataset.to_pandas()['Survived'].values, dtype=torch.float)\n",
    "dependent_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef193d95-c2f7-47cb-9798-7c4a49a1fd8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([712])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependent_var.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faac882-da41-45a9-a4b2-9cc35fbcfb9a",
   "metadata": {},
   "source": [
    "The independent variables are the variables we will use to make the prediction. Note that we use a trick in mutiplying the Pandas DataFrame by 1 to convert booleans to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18bdd66b-2dd7-4d0e-9434-d4b185d39558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[22.0000,  0.0000,  0.0000,  2.0949,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n",
       "        [24.0000,  2.0000,  0.0000,  4.3108,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n",
       "        [24.0000,  0.0000,  0.0000,  5.4316,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  1.0000,  0.0000,  0.0000],\n",
       "        [32.0000,  0.0000,  0.0000,  4.3476,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
       "        [30.0000,  0.0000,  0.0000,  2.1102,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n",
       "        [18.0000,  0.0000,  2.0000,  2.6391,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
       "        [47.0000,  1.0000,  1.0000,  3.9807,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
       "        [34.0000,  0.0000,  0.0000,  2.4423,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
       "        [39.0000,  1.0000,  1.0000,  4.7175,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
       "        [50.0000,  0.0000,  1.0000,  5.5155,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [20.0000,  1.0000,  0.0000,  2.3819,  0.0000,  0.0000,  1.0000,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
       "        [31.0000,  0.0000,  0.0000,  2.1691,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000],\n",
       "        [24.0000,  2.0000,  3.0000,  2.9832,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
       "        [21.0000,  0.0000,  0.0000,  2.1102,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n",
       "        [50.0000,  0.0000,  0.0000,  2.4423,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
       "        [22.0000,  0.0000,  0.0000,  2.1691,  0.0000,  0.0000,  1.0000,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
       "        [45.0000,  0.0000,  0.0000,  2.6741,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
       "        [33.0000,  1.0000,  1.0000,  3.0692,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n",
       "        [40.0000,  0.0000,  0.0000,  5.0400,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
       "        [24.0000,  0.0000,  0.0000,  2.1459,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "independent_cols = ['Age', 'SibSp', 'Parch', 'LogFare', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Sex_female', 'Sex_male',\n",
    "                    'Embarked_C', 'Embarked_Q', 'Embarked_S']\n",
    "\n",
    "independent_vars = tensor((train_dataset.to_pandas()*1)[independent_cols].values, dtype=torch.float)\n",
    "independent_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56b4dfcd-b5b4-4ea6-8a80-d581396f41b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([712, 12])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "independent_vars.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3fcf76-c29a-4cd5-8d01-84c4e5196247",
   "metadata": {},
   "source": [
    "### Normalise Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba95ae83-5b4a-4198-a8f6-d527714d8c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2973, 0.0000, 0.0000, 0.3357, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n",
       "        [0.3243, 0.2500, 0.0000, 0.6907, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n",
       "        [0.3243, 0.0000, 0.0000, 0.8703, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000],\n",
       "        [0.4324, 0.0000, 0.0000, 0.6966, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
       "        [0.4054, 0.0000, 0.0000, 0.3381, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n",
       "        [0.2432, 0.0000, 0.3333, 0.4229, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
       "        [0.6351, 0.1250, 0.1667, 0.6378, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
       "        [0.4595, 0.0000, 0.0000, 0.3913, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
       "        [0.5270, 0.1250, 0.1667, 0.7559, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
       "        [0.6757, 0.0000, 0.1667, 0.8838, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.2703, 0.1250, 0.0000, 0.3817, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
       "        [0.4189, 0.0000, 0.0000, 0.3476, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000],\n",
       "        [0.3243, 0.2500, 0.5000, 0.4780, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
       "        [0.2838, 0.0000, 0.0000, 0.3381, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n",
       "        [0.6757, 0.0000, 0.0000, 0.3913, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
       "        [0.2973, 0.0000, 0.0000, 0.3476, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
       "        [0.6081, 0.0000, 0.0000, 0.4285, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
       "        [0.4459, 0.1250, 0.1667, 0.4918, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n",
       "        [0.5405, 0.0000, 0.0000, 0.8076, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
       "        [0.3243, 0.0000, 0.0000, 0.3438, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_vals,indices = independent_vars.max(dim=0)\n",
    "independent_vars /= max_vals\n",
    "independent_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c19fa10-2f6d-43e8-8100-def837cde605",
   "metadata": {},
   "source": [
    "## Train Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82078812-7143-40fe-9e74-f32670dac63f",
   "metadata": {},
   "source": [
    "### Set Up Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5859de46-3042-4423-b4ca-56a194c867d9",
   "metadata": {},
   "source": [
    "Initialise random coefficients as a column vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1927f9ef-0db6-4418-b3cb-28c7129fe2c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1130],\n",
       "        [-0.4899],\n",
       "        [-0.1016],\n",
       "        [-0.4597],\n",
       "        [-0.3437],\n",
       "        [-0.0175],\n",
       "        [ 0.2362],\n",
       "        [-0.0940],\n",
       "        [ 0.0189],\n",
       "        [-0.2133],\n",
       "        [-0.2584],\n",
       "        [ 0.4228]], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_coeffs = independent_vars.shape[1]\n",
    "torch.manual_seed(42)\n",
    "coeffs = torch.rand(num_coeffs, 1) - 0.5\n",
    "coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf1bc4a-2080-47f0-ab32-c49f1851efd4",
   "metadata": {},
   "source": [
    "Transpose the dependent variable into a column vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d17e977-f786-411d-b69a-1ff419cb05e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependent_var = dependent_var[:,None]\n",
    "dependent_var[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b588ef8-8ac8-496a-89a3-29fb778016db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5573],\n",
       "        [ 0.0208],\n",
       "        [-0.9016],\n",
       "        [-0.9224],\n",
       "        [ 0.5683],\n",
       "        [ 0.1105],\n",
       "        [-0.3145],\n",
       "        [ 0.1833],\n",
       "        [-1.0172],\n",
       "        [-0.9979]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = independent_vars@coeffs\n",
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b044342-370b-4fd2-a0e0-c6e6a6c76c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6676, device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.abs(predictions - dependent_var).mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d17c8a05-65df-476a-a3e6-639094683c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_predictions(coeffs, independent_vars):\n",
    "    return torch.sigmoid(independent_vars@coeffs).sum(axis=1)\n",
    "\n",
    "def calc_loss(coeffs, independent_vars, dependent_var):\n",
    "    return torch.abs(calc_predictions(coeffs, independent_vars) - dependent_var).mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a6fd49-6af5-4d39-82d1-edf765b6434c",
   "metadata": {},
   "source": [
    "### Set Up Gradient Descent Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae0997df-6ae7-4081-a255-3bb99e360ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1130],\n",
       "        [-0.4899],\n",
       "        [-0.1016],\n",
       "        [-0.4597],\n",
       "        [-0.3437],\n",
       "        [-0.0175],\n",
       "        [ 0.2362],\n",
       "        [-0.0940],\n",
       "        [ 0.0189],\n",
       "        [-0.2133],\n",
       "        [-0.2584],\n",
       "        [ 0.4228]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeffs.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f0a6a13-e5dd-4ecd-9da0-114a57149df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5034, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = calc_loss(coeffs, independent_vars, dependent_var)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "764db880-8b78-4e2b-bd5a-7feed49d3b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0239],\n",
       "        [0.0041],\n",
       "        [0.0039],\n",
       "        [0.0296],\n",
       "        [0.0143],\n",
       "        [0.0138],\n",
       "        [0.0348],\n",
       "        [0.0213],\n",
       "        [0.0416],\n",
       "        [0.0118],\n",
       "        [0.0058],\n",
       "        [0.0453]], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.backward()\n",
    "coeffs.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8d334b3-d06e-41e0-b642-416916b0d158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5019, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "loss = calc_loss(coeffs, independent_vars, dependent_var)\n",
    "loss.backward()\n",
    "with torch.no_grad():\n",
    "    coeffs.sub_(coeffs.grad * 0.1)\n",
    "    coeffs.grad.zero_()\n",
    "    print(calc_loss(coeffs, independent_vars, dependent_var))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ceee0f-04ab-4e66-be71-0d8d7f5aea3f",
   "metadata": {},
   "source": [
    "### Run Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fe3a0b4-da48-46b4-9034-3966e4dbeff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_coeffs(coeffs, learning_rate):\n",
    "    coeffs.sub_(coeffs.grad * learning_rate)\n",
    "    coeffs.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6684e67-08d9-42cb-9a60-4d79ba748aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_epoch(coeffs, learning_rate):\n",
    "    loss = calc_loss(coeffs, independent_vars, dependent_var)\n",
    "    loss.backward()\n",
    "    with torch.no_grad(): update_coeffs(coeffs, learning_rate)\n",
    "    print(f'{loss:.3f}', end='; ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d58c531-4b82-4936-b2f1-1eb11554f5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_coeffs():\n",
    "    return (torch.rand(num_coeffs, 1) - 0.5).requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9af7154d-6074-45ba-9100-d9a1babccec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs=30, learning_rate=0.01):\n",
    "    torch.manual_seed(442)\n",
    "    coeffs = init_coeffs()\n",
    "    for i in range (epochs):\n",
    "        one_epoch(coeffs, learning_rate)\n",
    "        \n",
    "    return coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c80e4c8-0369-47b5-b58e-0f00c46aaeda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.517; 0.368; 0.368; 0.368; 0.368; 0.368; 0.368; 0.368; 0.368; 0.368; 0.368; 0.368; 0.368; 0.368; 0.368; 0.368; 0.368; 0.368; 0.368; 0.368; "
     ]
    }
   ],
   "source": [
    "coeffs = train_model(epochs=20, learning_rate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7706e362-bbad-4a8f-8264-6be55227156b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Age': tensor([-2.0584], device='cuda:0'),\n",
       " 'SibSp': tensor([-0.8960], device='cuda:0'),\n",
       " 'Parch': tensor([-0.2124], device='cuda:0'),\n",
       " 'LogFare': tensor([-2.8352], device='cuda:0'),\n",
       " 'Pclass_1': tensor([-1.3458], device='cuda:0'),\n",
       " 'Pclass_2': tensor([-1.7492], device='cuda:0'),\n",
       " 'Pclass_3': tensor([-3.9983], device='cuda:0'),\n",
       " 'Sex_female': tensor([-2.1495], device='cuda:0'),\n",
       " 'Sex_male': tensor([-3.9429], device='cuda:0'),\n",
       " 'Embarked_C': tensor([-0.9537], device='cuda:0'),\n",
       " 'Embarked_Q': tensor([-0.3663], device='cuda:0'),\n",
       " 'Embarked_S': tensor([-4.4945], device='cuda:0')}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def show_coeffs():\n",
    "    return dict(zip(independent_cols, coeffs.requires_grad_(False)))\n",
    "\n",
    "show_coeffs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a8b6cc",
   "metadata": {},
   "source": [
    "## Train Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bc5c1c",
   "metadata": {},
   "source": [
    "### Set up Coefficients\n",
    "\n",
    "Initialisation of the coefficients for the neural network is similar to the linear model. However, we need to initialise the coefficients for each layer in the network. We do this by creating a list of tensors, one for each layer. The number of columns in the first layer is equal to the number of independent variables. The number of columns in each subsequent layer is equal to the number of hidden coefficients, which we set to a constant value. The number of columns in the last layer is equal to the number of dependent variables i.e. 1 in our case.\n",
    "\n",
    "We divide the number of hidden coefficients by the number of columns in the first layer to ensure that the weights are distributed evenly across each layer. This ensures that the gradients for each layer will be similar and have a similar impact on the model's performance.\n",
    "\n",
    "We add a constant to the last layer of coefficients so that the model can learn a bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7b50490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_coeffs(num_hidden_coeffs=20):\n",
    "    layer_1 = (torch.rand(num_coeffs, num_hidden_coeffs) - 0.5) / num_hidden_coeffs\n",
    "    layer_2 = torch.rand(num_hidden_coeffs, 1) - 0.3\n",
    "    const = torch.rand(1)[0]\n",
    "    return layer_1.requires_grad_(), layer_2.requires_grad_(), const.requires_grad_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefb84c6",
   "metadata": {},
   "source": [
    "Calculation of predictions is also similar to the linear model except that we take the outputs from the first layer and pass them to the second layer. Note the use of `relu` to rectify the outputs of the first layer to ensure that they are positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d916c1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def calc_predictions(coeffs, independent_vars):\n",
    "  layer_1, layer_2, const = coeffs\n",
    "  result = F.relu(independent_vars@layer_1)\n",
    "  result = result@layer_2 + const\n",
    "  return torch.sigmoid(result)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb6561a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_coeffs(coeffs, learning_rate):\n",
    "  for layer_coeffs in coeffs:\n",
    "    layer_coeffs.sub_(layer_coeffs.grad * learning_rate)\n",
    "    layer_coeffs.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4b6a40",
   "metadata": {},
   "source": [
    "### Run Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e1dc7956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.543; 0.417; 0.271; 0.392; 0.226; 0.195; 0.188; 0.186; 0.186; 0.185; 0.185; 0.185; 0.185; 0.185; 0.185; 0.185; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; "
     ]
    }
   ],
   "source": [
    "coeffs = train_model(epochs=100, learning_rate=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a287fe5",
   "metadata": {},
   "source": [
    "## Train Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dfa42a",
   "metadata": {},
   "source": [
    "### Set up Coefficients\n",
    "\n",
    "We now genericise the number of hidden layers and their sizes. We also add a constant term to each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eff6350f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_coeffs(num_hidden_coeffs=[10, 10]):\n",
    "  layer_sizes = [num_coeffs] + num_hidden_coeffs + [1]\n",
    "  num_layers = len(layer_sizes)\n",
    "\n",
    "  layers = [(torch.rand(layer_sizes[i], layer_sizes[i+1]) - 0.5) / layer_sizes[i + 1] * 4 for i in range(num_layers - 1)]\n",
    "  consts = [(torch.rand(1)[0] - 0.5) * 0.1 for i in range(num_layers - 1)]\n",
    "\n",
    "  for layer in layers + consts:\n",
    "    layer.requires_grad_()\n",
    "\n",
    "  return layers, consts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f1c0de",
   "metadata": {},
   "source": [
    "Calculation of predictions proceeds largely as before but instead we loop over each layer rather than explicitly writing them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b95bf7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_predictions(coeffs, independent_vars):\n",
    "  layers, consts = coeffs\n",
    "  num_layers = len(layers)\n",
    "  result = independent_vars\n",
    "  \n",
    "  for i, layer in enumerate(layers):\n",
    "    result = result@layer + consts[i]\n",
    "    if i != num_layers - 1:\n",
    "      result = F.relu(result)\n",
    "\n",
    "  return torch.sigmoid(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ee050c",
   "metadata": {},
   "source": [
    "A minor change to to the training loop is required as we now have a list of coefficients rather than a single set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8fc352aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_coeffs(coeffs, learning_rate):\n",
    "  layers, consts = coeffs\n",
    "  for layer in layers + consts:\n",
    "    layer.sub_(layer.grad * learning_rate)\n",
    "    layer.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce540c16",
   "metadata": {},
   "source": [
    "### Run Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "276d16e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.505; 0.477; 0.403; 0.367; 0.367; 0.366; 0.365; 0.361; 0.349; 0.339; 0.338; 0.332; 0.310; 0.294; 0.254; 0.216; 0.204; 0.208; 0.199; 0.212; 0.193; 0.195; 0.205; 0.189; 0.188; 0.188; 0.187; 0.187; 0.187; 0.186; 0.186; 0.186; 0.186; 0.186; 0.186; 0.185; 0.185; 0.185; 0.185; 0.185; 0.185; 0.185; 0.185; 0.185; 0.185; 0.185; 0.185; 0.185; 0.185; 0.185; 0.185; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.184; 0.183; 0.183; 0.183; 0.183; 0.183; 0.183; 0.183; 0.183; 0.183; 0.183; 0.183; 0.183; 0.183; 0.183; 0.183; 0.183; 0.183; 0.183; 0.183; 0.183; 0.183; 0.183; 0.183; 0.183; 0.183; 0.183; "
     ]
    }
   ],
   "source": [
    "coeffs = train_model(epochs=100, learning_rate=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675cbb91",
   "metadata": {},
   "source": [
    "## Recreate as PyTorch Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c03100",
   "metadata": {},
   "source": [
    "We subclass the PyTorch Sequential class to create a model that we can train. The layers from the previous example are recreated as members of the class, and are initialised with random weights as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c238d09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TitanicModel(nn.Sequential):\n",
    "  def __init__(self):\n",
    "    super(TitanicModel, self).__init__(nn.Linear(num_coeffs, 10),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(10, 10),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(10, 1),\n",
    "                                       nn.Sigmoid())\n",
    "    \n",
    "    self.apply(self._init_weights)\n",
    "    \n",
    "  def _init_weights(self, module):\n",
    "    if isinstance(module, nn.Linear):\n",
    "      module.weight.data.normal_(mean=0.0, std=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456d4aca",
   "metadata": {},
   "source": [
    "The training loop is similar to before with the exception that we use the PyTorch optimisation machinery rather than creating our own.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d9ab947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train_model(epochs=30, learning_rate=10):\n",
    "  torch.manual_seed(442)\n",
    "  model = TitanicModel()\n",
    "  calc_loss = nn.L1Loss()\n",
    "  optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(independent_vars)\n",
    "    loss = calc_loss(output, dependent_var)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'{loss:.3f}', end='; ')\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabe3a01",
   "metadata": {},
   "source": [
    "Training progresses similar to before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "51a207ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.480; 0.349; 0.272; 0.320; 0.334; 0.278; 0.212; 0.209; 0.208; 0.207; 0.202; 0.198; 0.198; 0.200; 0.209; 0.204; 0.189; 0.187; 0.186; 0.186; 0.186; 0.184; 0.184; 0.184; 0.183; "
     ]
    },
    {
     "data": {
      "text/plain": [
       "TitanicModel(\n",
       "  (0): Linear(in_features=12, out_features=10, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=10, out_features=1, bias=True)\n",
       "  (5): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(epochs=25, learning_rate=12.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
